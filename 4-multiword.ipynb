{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import requests\n",
    "\n",
    "text_by_file = utils.load_files('./ustawy/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use SpaCy [tokenizer API](https://spacy.io/api/tokenizer) to tokenize the text from the law corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.pl import Polish\n",
    "\n",
    "for file,text in text_by_file.items():\n",
    "    text_by_file[file] = ' '.join(text.split())\n",
    "    \n",
    "\n",
    "nlp = Polish()\n",
    "tokenizer = nlp.tokenizer\n",
    "files = tokenizer.pipe(text_by_file.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute **bigram** counts of downcased tokens.  Given the sentence: \"The quick brown fox jumps over the\n",
    "   lazy dog.\", the bigram counts are as follows:\n",
    "   1. \"the quick\": 1\n",
    "   1. \"quick brown\": 1\n",
    "   1. \"brown fox\": 1\n",
    "   1. ...\n",
    "   1. \"dog .\": 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Discard bigrams containing characters other than letters. Make sure that you discard the invalid entries **after**\n",
    "   computing the bigram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use [pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) to compute the measure \n",
    "   for all pairs of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Sort the word pairs according to that measure in the descending order and determine top 10 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Filter bigrams with number of occurrences lower than 5. Determine top 10 entries for the remaining dataset (>=5\n",
    "   occurrences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use [KRNNT](https://hub.docker.com/r/djstrong/krnnt2) or Clarin-PL API(https://ws.clarin-pl.eu/tager.shtml) to tag and lemmatize the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Using the tagged corpus compute bigram statistic for the tokens containing:\n",
    "   a. lemmatized, downcased word\n",
    "   b. morphosyntactic category of the word (subst, fin, adj, etc.)\n",
    "9. For example: \"Ala ma kota\", which is tagged as:\n",
    "   ```\n",
    "   Ala\tnone\n",
    "           Ala\tsubst:sg:nom:f\tdisamb\n",
    "   ma\tspace\n",
    "           mieć\tfin:sg:ter:imperf\tdisamb\n",
    "   kota\tspace\n",
    "           kot\tsubst:sg:acc:m2\tdisamb\n",
    "   .\tnone\n",
    "           .\tinterp\tdisamb\n",
    "   ```\n",
    "   the algorithm should return the following bigrams: `ala:subst mieć:fin` and `mieć:fin kot:subst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Compute the same statistics as for the non-lemmatized words (i.e. PMI) and print top-10 entries with at least 5 occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Compute **trigram** counts for both corpora and perform the same filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Use PMI (with 5 occurrence threshold) to compute top 10 results for the trigrams. Devise a method for computing the values, based on the\n",
    "   results for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Create a table comparing the results for copora without and with tagging and lemmatization (separate table for bigrams and trigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Answer the following questions:\n",
    "      1. Why do we have to filter the bigrams, rather than the token sequence?\n",
    "      1. Which method works better for the bigrams and which for the trigrams?\n",
    "      1. What types of expressions are discovered by the methods.\n",
    "      1. Can you devise a different type of filtering that would yield better results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
